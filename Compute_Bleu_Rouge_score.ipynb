{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/grad3/hle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = np.load('./data/references_from_0_to_1000.npy', allow_pickle=True)\n",
    "expressions_1 = np.load('./data/top3_exps_from_0_to_1000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vase furthest left', 'far left vase', 'far left vase']\n",
      "[['vase', 'furthest', 'left'], ['far', 'left', 'vase'], ['far', 'left', 'vase']]\n",
      "['table' 'gray sitting flowers' 'wall']\n",
      "['gray', 'sitting', 'flowers']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_ref = []\n",
    "ref_1 = [r[0] for r in references[500]]\n",
    "print(ref_1)\n",
    "for r in ref_1:\n",
    "    lis=word_tokenize(r.lower())\n",
    "    tokenized_ref.append(lis)\n",
    "print(tokenized_ref)\n",
    "print(expressions_1[500])\n",
    "print(word_tokenize(expressions_1[500][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.36787944117144233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(tokenized_ref, ['shirt'])\n",
    "print(\"BLEU:\",BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "bleu_score = []\n",
    "files = [0,1000,2000,3000,4000]\n",
    "for start in files:\n",
    "    references = np.load(f'./data/references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'./data/generated_expressions/test/generated_expression_probabilisticutterance_lstm_adjusted/top3_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for ref, exps in zip(references, expressions):\n",
    "        item_reference = [word_tokenize(r[0].lower()) for r in ref]\n",
    "    #     print(item_reference)\n",
    "    #     print(exps)\n",
    "    #     print('$$$$$$$$$$$')\n",
    "\n",
    "        best_matched = max([sentence_bleu(item_reference, word_tokenize(exp)) for exp in exps])#[0]\n",
    "        bleu_score.append(best_matched)\n",
    "print(len(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6693761222323064 0.24604247525179773\n",
      "0.4392446114088394 0.37524437314968107\n",
      "3281\n"
     ]
    }
   ],
   "source": [
    "x = [score for score in bleu_score if score != 0]\n",
    "print(np.average(x), np.std(x))\n",
    "print(np.average(bleu_score), np.std(bleu_score))\n",
    "print(len(x))\n",
    "# 0.3939225254870882 0.3270738202276801\n",
    "# 0.1983005993302002 0.3043752780752566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATE ROUGE SCORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = np.load('./data/references_from_0_to_1000.npy', allow_pickle=True)\n",
    "expressions_1 = np.load('./data/top3_exps_from_0_to_1000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cat'], ['Cat on right'], ['cat, but not in reflection']] ['carpet' 'cat' 'dog']\n"
     ]
    }
   ],
   "source": [
    "cat_ref = references[1]\n",
    "cat_exps = expressions_1[1]\n",
    "print(cat_ref, cat_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=1.0, recall=1.0, fmeasure=1.0) Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "Score(precision=1.0, recall=0.3333333333333333, fmeasure=0.5) Score(precision=1.0, recall=0.3333333333333333, fmeasure=0.5)\n",
      "Score(precision=1.0, recall=0.2, fmeasure=0.33333333333333337) Score(precision=1.0, recall=0.2, fmeasure=0.33333333333333337)\n",
      "0.7074074074074074\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "for i in cat_ref:\n",
    "    score = scorer.score(i[0], cat_exps[1])\n",
    "    print(score['rouge1'], score['rougeL'])\n",
    "    test_scores.append(score['rouge1'])\n",
    "print(np.average(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "rouge1_scores = []\n",
    "rougeL_scores = []\n",
    "for start in files:\n",
    "    references = np.load(f'./data/references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'./data/top5_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for ref, exps in zip(references, expressions):\n",
    "        item_reference = [word_tokenize(r[0].lower()) for r in ref]\n",
    "        exp_recalls_rouge_1 = []\n",
    "        exp_recalls_rouge_L = []\n",
    "        for exp in exps:\n",
    "            exp_recalls_rouge_1.append(np.average([scorer.score(target[0], exp)['rouge1'].recall for target in ref]))\n",
    "            exp_recalls_rouge_L.append(np.average([scorer.score(target[0], exp)['rougeL'].recall for target in ref]))\n",
    "        rouge1_scores.append(max(exp_recalls_rouge_1))\n",
    "        rougeL_scores.append(max(exp_recalls_rouge_L))\n",
    "print(len(rouge1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24571196905987616 0.16725547757380804\n",
      "0.18330112891866762 0.17974669943941723\n"
     ]
    }
   ],
   "source": [
    "nonzero_rouge1 = [score for score in rouge1_scores if score != 0]\n",
    "print(np.average(nonzero_rouge1), np.std(nonzero_rouge1))\n",
    "print(np.average(rouge1_scores), np.std(rouge1_scores))\n",
    "# top3 :0.24809894991750528 0.16986040769537178\n",
    "#0.1687072859439036 0.18169670240599997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2480695381527994 0.1698341010559976\n",
      "0.1686872859439036 0.18167124031241783\n"
     ]
    }
   ],
   "source": [
    "nonzero_rougeL = [score for score in rougeL_scores if score != 0]\n",
    "print(np.average(nonzero_rougeL), np.std(nonzero_rougeL))\n",
    "print(np.average(rougeL_scores), np.std(rougeL_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "references = np.load('./data/references_from_0_to_1000.npy', allow_pickle=True)\n",
    "expressions_1 = np.load('./data/top3_exps_from_0_to_1000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add meteor metric (include synonims)\n",
    "#### Improve matching algorithm (prioritizing matched objects with synonims)\n",
    "#### Add relations to RSA ?\n",
    "#### Generate attributes/objects/relations from scenegraph on the training dataset.\n",
    "#### Learn threshold (entropy & for relations?) from the training data\n",
    "#### Idea: treating nodes as objects & edges as relations. -> generate expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396\n",
      "[12, 14, 24, 27, 30, 35, 36, 42, 43, 46]\n",
      "1270\n",
      "[12, 14, 24, 27, 30, 35, 36, 42, 43, 46]\n",
      "1270\n",
      "[12, 14, 24, 27, 30, 35, 36, 42, 43, 46]\n"
     ]
    }
   ],
   "source": [
    "# FINDING ALL IMAGES WITH 0 SCORE EXPRESSION\n",
    "bad_bleu_score_images = []\n",
    "for i in range(len(bleu_score)):\n",
    "    if bleu_score[i] == 0:\n",
    "        bad_bleu_score_images.append(i)\n",
    "print(len(bad_bleu_score_images))\n",
    "print(bad_bleu_score_images[:10])\n",
    "\n",
    "bad_rouge1_score_images = []\n",
    "bad_rougeL_score_images = []\n",
    "for i in range(len(rouge1_scores)):\n",
    "    if rouge1_scores[i] == 0:\n",
    "        bad_rouge1_score_images.append(i)\n",
    "    if rougeL_scores[i] == 0:\n",
    "        bad_rougeL_score_images.append(i)\n",
    "print(len(bad_rouge1_score_images))\n",
    "print(bad_rouge1_score_images[:10])\n",
    "print(len(bad_rougeL_score_images))\n",
    "print(bad_rougeL_score_images[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1270\n",
      "1270\n",
      "1396\n"
     ]
    }
   ],
   "source": [
    "rouge_bad_intersection = np.intersect1d(bad_rougeL_score_images, bad_bleu_score_images)\n",
    "print(len(rouge_bad_intersection))\n",
    "bleu_rouge_bad_intersection = np.intersect1d(bad_rougeL_score_images, bad_bleu_score_images)\n",
    "print(len(bleu_rouge_bad_intersection))\n",
    "bleu_rouge_bad_union = np.union1d(bad_rougeL_score_images, bad_bleu_score_images)\n",
    "print(len(bleu_rouge_bad_union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bad_rouge_score_reference_indexes.npy',bad_rouge1_score_images)\n",
    "np.save('bad_bleu_score_reference_indexes.npy',bad_bleu_score_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score as meteor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor(ref_1, \"person elephant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "meteor_score = []\n",
    "files = [0,1000,2000,3000,4000]\n",
    "for start in files:\n",
    "    references = np.load(f'./data/references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'./data/top5_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for ref, exps in zip(references, expressions):\n",
    "        item_reference = [r[0].lower() for r in ref]\n",
    "    #     print(item_reference)\n",
    "    #     print(exps)\n",
    "    #     print('$$$$$$$$$$$')\n",
    "        best_matched = max([meteor(item_reference, exp) for exp in exps])\n",
    "        meteor_score.append(best_matched)\n",
    "print(len(meteor_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2535472318146614 0.13964477796877744\n",
      "0.19158028835915816 0.1631145199695016\n",
      "3778\n"
     ]
    }
   ],
   "source": [
    "x = [score for score in meteor_score if score != 0]\n",
    "print(np.average(x), np.std(x))\n",
    "print(np.average(meteor_score), np.std(meteor_score))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUMPING LABELS TO JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [0,1000,2000,3000,4000]\n",
    "exps = {}\n",
    "for start in files:\n",
    "    references = np.load(f'./data/test/detectron2_with_target/references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'./data/test/detectron2_with_target/top3_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for i, expressions in enumerate(expressions):\n",
    "        exps[start+i] = expressions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blue jar'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exps[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_expressions_with_true_target.json', 'w') as fp:\n",
    "    json.dump(exps, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "bleu_score = []\n",
    "counter = 0\n",
    "\n",
    "references = np.load(f'./data/test/references_from_0_to_5000.npy', allow_pickle=True)\n",
    "expressions = np.load(f'./data/test/top3_exps_from_0_to_5000.npy', allow_pickle=True)\n",
    "for ref, exps in zip(references, expressions):\n",
    "    item_reference = [word_tokenize(r[0].lower()) for r in ref]\n",
    "#     print(item_reference)\n",
    "#     print(exps)\n",
    "#     print('$$$$$$$$$$$')\n",
    "\n",
    "    #best_matched = max([sentence_bleu(item_reference, word_tokenize(exp)) for exp in exps])#[0]\n",
    "    best_matched = sentence_bleu(item_reference, word_tokenize(exps[0]))\n",
    "    bleu_score.append(best_matched)\n",
    "print(len(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6618485505995223 0.25576376768967624\n",
      "0.30034687226206314 0.37183500775597955\n",
      "2269\n"
     ]
    }
   ],
   "source": [
    "non_zero_entries = [score for score in bleu_score if score != 0]\n",
    "print(np.average(non_zero_entries), np.std(non_zero_entries))\n",
    "print(np.average(bleu_score), np.std(bleu_score))\n",
    "print(len(non_zero_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18100858265645028 0.19190863322770094\n",
      "0.17744893184429947 0.18789276425550427\n"
     ]
    }
   ],
   "source": [
    "rouge1_scores = []\n",
    "rougeL_scores = []\n",
    "references = np.load(f'./data/generated_expressions/test/generated_expression_binary_utterance_lstm_adjusted/references_from_0_to_5000.npy', allow_pickle=True)\n",
    "expressions = np.load(f'./data/generated_expressions/test/generated_expression_binary_utterance_lstm_adjusted/top3_exps_from_0_to_5000.npy', allow_pickle=True)\n",
    "for ref, exps in zip(references, expressions):\n",
    "    item_reference = [word_tokenize(r[0].lower()) for r in ref]\n",
    "    exp_recalls_rouge_1 = []\n",
    "    exp_recalls_rouge_L = []\n",
    "    for exp in exps:\n",
    "        exp_recalls_rouge_1.append(np.average([scorer.score(target[0], exp)['rouge1'].recall for target in ref]))\n",
    "        exp_recalls_rouge_L.append(np.average([scorer.score(target[0], exp)['rougeL'].recall for target in ref]))\n",
    "    rouge1_scores.append(max(exp_recalls_rouge_1))\n",
    "    rougeL_scores.append(max(exp_recalls_rouge_L))\n",
    "print(np.average(rouge1_scores), np.std(rouge1_scores))\n",
    "print(np.average(rougeL_scores), np.std(rougeL_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "0.25260105199765515 0.15058741794027786\n",
      "0.1787910246039403 0.1710176803215331\n",
      "3539\n"
     ]
    }
   ],
   "source": [
    "meteor_score = []\n",
    "\n",
    "references = np.load(f'./data/generated_expressions/test/generated_expression_binary_utterance_lstm_adjusted/references_from_0_to_5000.npy', allow_pickle=True)\n",
    "expressions = np.load(f'./data/generated_expressions/test/generated_expression_binary_utterance_lstm_adjusted/top3_exps_from_0_to_5000.npy', allow_pickle=True)\n",
    "for ref, exps in zip(references, expressions):\n",
    "    item_reference = [r[0].lower() for r in ref]\n",
    "#     print(item_reference)\n",
    "#     print(exps)\n",
    "#     print('$$$$$$$$$$$')\n",
    "    best_matched = max([meteor(item_reference, exp) for exp in exps])\n",
    "    meteor_score.append(best_matched)\n",
    "print(len(meteor_score))\n",
    "x = [score for score in meteor_score if score != 0]\n",
    "print(np.average(x), np.std(x))\n",
    "print(np.average(meteor_score), np.std(meteor_score))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "f = open('generated_expression_probabilistic_utterance_lstm.json')\n",
    "dat = json.load(f)\n",
    "long_exps = 0\n",
    "for i in range(5000):\n",
    "    if \" \" in dat[str(i)]:\n",
    "        long_exps += 1\n",
    "print(long_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3577\n"
     ]
    }
   ],
   "source": [
    "exps = {}\n",
    "long_exps = 0\n",
    "expressions = np.load(f'./data/test/top3_exps_from_0_to_5000.npy', allow_pickle=True)\n",
    "for i, expressions in enumerate(expressions):\n",
    "    exps[i] = expressions[0]\n",
    "    if \" \" in expressions[0]:\n",
    "        long_exps += 1\n",
    "print(long_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_expressions.json', 'w') as fp:\n",
    "    json.dump(exps, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/scratch2/hle/py3_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "bleu_score = []\n",
    "files = [0,1000,2000,3000,4000]\n",
    "for start in files:\n",
    "    references = np.load(f'./data/test/references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'./data/test/top3_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for ref, exps in zip(references, expressions):\n",
    "        item_reference = [word_tokenize(r[0].lower()) for r in ref]\n",
    "    #     print(item_reference)\n",
    "    #     print(exps)\n",
    "    #     print('$$$$$$$$$$$')\n",
    "\n",
    "#         best_matched = max([sentence_bleu(item_reference, word_tokenize(exp)) for exp in exps])#[0]\n",
    "        best_matched = sentence_bleu(item_reference, word_tokenize(exps[0]))\n",
    "        bleu_score.append(best_matched)\n",
    "print(len(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6629174540870093 0.25348291217850427\n",
      "0.3008319406646848 0.3715981234667934\n",
      "2269\n"
     ]
    }
   ],
   "source": [
    "non_zero_entries = [score for score in bleu_score if score != 0]\n",
    "print(np.average(non_zero_entries), np.std(non_zero_entries))\n",
    "print(np.average(bleu_score), np.std(bleu_score))\n",
    "print(len(non_zero_entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATE WITH REFCOCO API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions = np.load(f'./data/generated_expressions/test/generated_expression_binary_utterance_lstm_adjusted/top3_exps_from_0_to_5000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the left woman', 'brown man', 'purple shirt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expressions[137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scratch2/hle/git/refer\")\n",
    "from refer import REFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset refcoco into memory...\n",
      "creating index...\n",
      "index created.\n",
      "DONE (t=6.39s)\n"
     ]
    }
   ],
   "source": [
    "refer = REFER(\"/scratch2/hle/git/refer/data\",\"refcoco\", \"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset refcoco into memory...\n",
      "creating index...\n",
      "index created.\n",
      "DONE (t=11.82s)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scratch2/hle/git/refer\")\n",
    "sys.path.append(\"/scratch2/hle/git/rsa_refer\")\n",
    "sys.path.append('/scratch2/hle/git/refer/evaluation')\n",
    "sys.path.append('/scratch2/hle/git/refer/evaluation/bleu')\n",
    "sys.path.append('/scratch2/hle/git/refer/evaluation/cider')\n",
    "sys.path.append('/scratch2/hle/git/refer/evaluation/meteor')\n",
    "sys.path.append('/scratch2/hle/git/refer/evaluation/rouge')\n",
    "sys.path.append('/scratch2/hle/git/refer/evaluation/tokenizer')\n",
    "from refEvaluation import RefEvaluation\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from refer import REFER\n",
    "refer = REFER('/scratch2/hle/git/refer/data', 'refcoco', 'google')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('done', 0)\n",
      "('done', 100)\n",
      "('done', 200)\n",
      "('done', 300)\n",
      "('done', 400)\n",
      "('done', 500)\n",
      "('done', 600)\n",
      "('done', 700)\n",
      "('done', 800)\n",
      "('done', 900)\n",
      "('done', 1000)\n",
      "('done', 1100)\n",
      "('done', 1200)\n",
      "('done', 1300)\n",
      "('done', 1400)\n",
      "('done', 1500)\n",
      "('done', 1600)\n",
      "('done', 1700)\n",
      "('done', 1800)\n",
      "('done', 1900)\n",
      "('done', 2000)\n",
      "('done', 2100)\n",
      "('done', 2200)\n",
      "('done', 2300)\n",
      "('done', 2400)\n",
      "('done', 2500)\n",
      "('done', 2600)\n",
      "('done', 2700)\n",
      "('done', 2800)\n",
      "('done', 2900)\n",
      "('done', 3000)\n",
      "('done', 3100)\n",
      "('done', 3200)\n",
      "('done', 3300)\n",
      "('done', 3400)\n",
      "('done', 3500)\n",
      "('done', 3600)\n",
      "('done', 3700)\n",
      "('done', 3800)\n",
      "('done', 3900)\n",
      "('done', 4000)\n",
      "('done', 4100)\n",
      "('done', 4200)\n",
      "('done', 4300)\n",
      "('done', 4400)\n",
      "('done', 4500)\n",
      "('done', 4600)\n",
      "('done', 4700)\n",
      "('done', 4800)\n",
      "('done', 4900)\n"
     ]
    }
   ],
   "source": [
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "data_path = config['data_path']\n",
    "f = open('generated_expressions.json')\n",
    "expressions = json.load(f)#np.load('/scratch2/hle/git/rsa_refer/data/generated_expressions/test/generated_expression_binary_utterance_lstm_adjusted/top3_exps_from_0_to_5000.npy', allow_pickle=True)\n",
    "eval_expressions = []\n",
    "for i in range(5000):\n",
    "    df = pd.read_csv(os.path.join(data_path,'refCOCO/test/attr_tables/attr_'+str(i)+'.tsv'), encoding='utf-8',sep='\\t')\n",
    "\n",
    "    ref_id = df['ref_id'][0]\n",
    "    eval_expressions.append({'sent': expressions[str(i)], 'ref_id':ref_id})\n",
    "    if i % 100 == 0:\n",
    "        print('done', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'reflen': 13272, 'guess': [10533, 5533, 1956, 1063], 'testlen': 10533, 'correct': [2508, 52, 2, 0]}\n",
      "ratio: 0.793625678119\n",
      "Bleu_1: 0.184\n",
      "Bleu_2: 0.036\n",
      "Bleu_3: 0.010\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n",
      "METEOR: 0.079\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.197\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.317\n"
     ]
    }
   ],
   "source": [
    "refEval = RefEvaluation(refer, eval_expressions)\n",
    "refEval.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
