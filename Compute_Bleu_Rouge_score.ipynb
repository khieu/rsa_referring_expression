{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/grad3/hle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = np.load('./data/references_from_0_to_1000.npy', allow_pickle=True)\n",
    "expressions_1 = np.load('./data/top3_exps_from_0_to_1000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guy petting elephant', 'foremost person', 'green shirt']\n",
      "[['guy', 'petting', 'elephant'], ['foremost', 'person'], ['green', 'shirt']]\n",
      "['sidewalk' 'man' 'ground']\n",
      "['man']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_ref = []\n",
    "ref_1 = [r[0] for r in references[0]]\n",
    "print(ref_1)\n",
    "for r in ref_1:\n",
    "    lis=word_tokenize(r.lower())\n",
    "    tokenized_ref.append(lis)\n",
    "print(tokenized_ref)\n",
    "print(expressions_1[0])\n",
    "print(word_tokenize(expressions_1[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.36787944117144233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch3/hle/conda_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(tokenized_ref, ['shirt'])\n",
    "print(\"BLEU:\",BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch3/hle/conda_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/scratch3/hle/conda_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "bleu_score = []\n",
    "counter = 0\n",
    "files = [0,1000,2000,3000,4000]\n",
    "for start in files:\n",
    "    references = np.load(f'./data/references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'./data/top5_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for ref, exps in zip(references, expressions):\n",
    "        item_reference = [word_tokenize(r[0].lower()) for r in ref]\n",
    "    #     print(item_reference)\n",
    "    #     print(exps)\n",
    "    #     print('$$$$$$$$$$$')\n",
    "\n",
    "        best_matched = max([sentence_bleu(item_reference, word_tokenize(exp)) for exp in exps])\n",
    "        bleu_score.append(best_matched)\n",
    "print(len(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37909704078386836 0.31737472828238644\n",
      "0.2732531469970123 0.31863144149921424\n",
      "3604\n"
     ]
    }
   ],
   "source": [
    "x = [score for score in bleu_score if score != 0]\n",
    "print(np.average(x), np.std(x))\n",
    "print(np.average(bleu_score), np.std(bleu_score))\n",
    "print(len(x))\n",
    "# 0.3939225254870882 0.3270738202276801\n",
    "# 0.1983005993302002 0.3043752780752566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATE ROUGE SCORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = np.load('./data/references_from_0_to_1000.npy', allow_pickle=True)\n",
    "expressions_1 = np.load('./data/top3_exps_from_0_to_1000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cat'], ['Cat on right'], ['cat, but not in reflection']] ['carpet' 'cat' 'dog']\n"
     ]
    }
   ],
   "source": [
    "cat_ref = references[1]\n",
    "cat_exps = expressions_1[1]\n",
    "print(cat_ref, cat_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=1.0, recall=1.0, fmeasure=1.0) Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "Score(precision=1.0, recall=0.3333333333333333, fmeasure=0.5) Score(precision=1.0, recall=0.3333333333333333, fmeasure=0.5)\n",
      "Score(precision=1.0, recall=0.2, fmeasure=0.33333333333333337) Score(precision=1.0, recall=0.2, fmeasure=0.33333333333333337)\n",
      "0.7074074074074074\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "for i in cat_ref:\n",
    "    score = scorer.score(i[0], cat_exps[1])\n",
    "    print(score['rouge1'], score['rougeL'])\n",
    "    test_scores.append(score['rouge1'])\n",
    "print(np.average(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "rouge1_scores = []\n",
    "rougeL_scores = []\n",
    "for start in files:\n",
    "    references = np.load(f'./data/references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'./data/top5_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for ref, exps in zip(references, expressions):\n",
    "        item_reference = [word_tokenize(r[0].lower()) for r in ref]\n",
    "        exp_recalls_rouge_1 = []\n",
    "        exp_recalls_rouge_L = []\n",
    "        for exp in exps:\n",
    "            exp_recalls_rouge_1.append(np.average([scorer.score(target[0], exp)['rouge1'].recall for target in ref]))\n",
    "            exp_recalls_rouge_L.append(np.average([scorer.score(target[0], exp)['rougeL'].recall for target in ref]))\n",
    "        rouge1_scores.append(max(exp_recalls_rouge_1))\n",
    "        rougeL_scores.append(max(exp_recalls_rouge_L))\n",
    "print(len(rouge1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24571196905987616 0.16725547757380804\n",
      "0.18330112891866762 0.17974669943941723\n"
     ]
    }
   ],
   "source": [
    "nonzero_rouge1 = [score for score in rouge1_scores if score != 0]\n",
    "print(np.average(nonzero_rouge1), np.std(nonzero_rouge1))\n",
    "print(np.average(rouge1_scores), np.std(rouge1_scores))\n",
    "# top3 :0.24809894991750528 0.16986040769537178\n",
    "#0.1687072859439036 0.18169670240599997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2480695381527994 0.1698341010559976\n",
      "0.1686872859439036 0.18167124031241783\n"
     ]
    }
   ],
   "source": [
    "nonzero_rougeL = [score for score in rougeL_scores if score != 0]\n",
    "print(np.average(nonzero_rougeL), np.std(nonzero_rougeL))\n",
    "print(np.average(rougeL_scores), np.std(rougeL_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "references = np.load('./data/references_from_0_to_1000.npy', allow_pickle=True)\n",
    "expressions_1 = np.load('./data/top3_exps_from_0_to_1000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add meteor metric (include synonims)\n",
    "#### Improve matching algorithm (prioritizing matched objects with synonims)\n",
    "#### Add relations to RSA ?\n",
    "#### Generate attributes/objects/relations from scenegraph on the training dataset.\n",
    "#### Learn threshold (entropy & for relations?) from the training data\n",
    "#### Idea: treating nodes as objects & edges as relations. -> generate expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396\n",
      "[12, 14, 24, 27, 30, 35, 36, 42, 43, 46]\n",
      "1270\n",
      "[12, 14, 24, 27, 30, 35, 36, 42, 43, 46]\n",
      "1270\n",
      "[12, 14, 24, 27, 30, 35, 36, 42, 43, 46]\n"
     ]
    }
   ],
   "source": [
    "# FINDING ALL IMAGES WITH 0 SCORE EXPRESSION\n",
    "bad_bleu_score_images = []\n",
    "for i in range(len(bleu_score)):\n",
    "    if bleu_score[i] == 0:\n",
    "        bad_bleu_score_images.append(i)\n",
    "print(len(bad_bleu_score_images))\n",
    "print(bad_bleu_score_images[:10])\n",
    "\n",
    "bad_rouge1_score_images = []\n",
    "bad_rougeL_score_images = []\n",
    "for i in range(len(rouge1_scores)):\n",
    "    if rouge1_scores[i] == 0:\n",
    "        bad_rouge1_score_images.append(i)\n",
    "    if rougeL_scores[i] == 0:\n",
    "        bad_rougeL_score_images.append(i)\n",
    "print(len(bad_rouge1_score_images))\n",
    "print(bad_rouge1_score_images[:10])\n",
    "print(len(bad_rougeL_score_images))\n",
    "print(bad_rougeL_score_images[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1270\n",
      "1270\n",
      "1396\n"
     ]
    }
   ],
   "source": [
    "rouge_bad_intersection = np.intersect1d(bad_rougeL_score_images, bad_bleu_score_images)\n",
    "print(len(rouge_bad_intersection))\n",
    "bleu_rouge_bad_intersection = np.intersect1d(bad_rougeL_score_images, bad_bleu_score_images)\n",
    "print(len(bleu_rouge_bad_intersection))\n",
    "bleu_rouge_bad_union = np.union1d(bad_rougeL_score_images, bad_bleu_score_images)\n",
    "print(len(bleu_rouge_bad_union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bad_rouge_score_reference_indexes.npy',bad_rouge1_score_images)\n",
    "np.save('bad_bleu_score_reference_indexes.npy',bad_bleu_score_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score as meteor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor(ref_1, \"person elephant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "meteor_score = []\n",
    "counter = 0\n",
    "files = [0,1000,2000,3000,4000]\n",
    "for start in files:\n",
    "    references = np.load(f'./data/references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'./data/top5_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for ref, exps in zip(references, expressions):\n",
    "        item_reference = [r[0].lower() for r in ref]\n",
    "    #     print(item_reference)\n",
    "    #     print(exps)\n",
    "    #     print('$$$$$$$$$$$')\n",
    "        best_matched = max([meteor(item_reference, exp) for exp in exps])\n",
    "        meteor_score.append(best_matched)\n",
    "print(len(meteor_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2535472318146614 0.13964477796877744\n",
      "0.19158028835915816 0.1631145199695016\n",
      "3778\n"
     ]
    }
   ],
   "source": [
    "x = [score for score in meteor_score if score != 0]\n",
    "print(np.average(x), np.std(x))\n",
    "print(np.average(meteor_score), np.std(meteor_score))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
