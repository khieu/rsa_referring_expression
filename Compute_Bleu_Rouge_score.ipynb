{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/grad3/hle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = np.load('references_from_0_to_1000.npy', allow_pickle=True)\n",
    "expressions_1 = np.load('top3_exps_from_0_to_1000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'Cat on right', 'cat, but not in reflection']\n",
      "[['cat'], ['cat', 'on', 'right'], ['cat', ',', 'but', 'not', 'in', 'reflection']]\n",
      "['carpet' 'cat' 'dog']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_ref = []\n",
    "ref_1 = [r[0] for r in references[1]]\n",
    "print(ref_1)\n",
    "for r in ref_1:\n",
    "    lis=word_tokenize(r.lower())\n",
    "    tokenized_ref.append(lis)\n",
    "print(tokenized_ref)\n",
    "print(expressions_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch3/hle/conda_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(tokenized_ref, word_tokenize(expressions_1[1][1]))\n",
    "print(\"BLEU:\",BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch3/hle/conda_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/scratch3/hle/conda_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "bleu_score = []\n",
    "counter = 0\n",
    "files = [0,1000,2000,3000,4000]\n",
    "for start in files:\n",
    "    references = np.load(f'references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'top3_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for ref, exps in zip(references, expressions):\n",
    "        item_reference = [word_tokenize(r[0].lower()) for r in ref]\n",
    "    #     print(item_reference)\n",
    "    #     print(exps)\n",
    "    #     print('$$$$$$$$$$$')\n",
    "\n",
    "        best_matched = max([sentence_bleu(item_reference, word_tokenize(exp)) for exp in exps])\n",
    "        bleu_score.append(best_matched)\n",
    "print(len(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38507907917574985 0.3187066035683757\n",
      "0.24899213259503986 0.3155348827674987\n",
      "3233\n"
     ]
    }
   ],
   "source": [
    "x = [score for score in bleu_score if score != 0]\n",
    "print(np.average(x), np.std(x))\n",
    "print(np.average(bleu_score), np.std(bleu_score))\n",
    "print(len(x))\n",
    "# 0.3939225254870882 0.3270738202276801\n",
    "# 0.1983005993302002 0.3043752780752566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATE ROUGE SCORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = np.load('references_from_0_to_1000.npy', allow_pickle=True)\n",
    "expressions_1 = np.load('top3_exps_from_0_to_1000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cat'], ['Cat on right'], ['cat, but not in reflection']] ['carpet' 'cat' 'dog']\n"
     ]
    }
   ],
   "source": [
    "cat_ref = references[1]\n",
    "cat_exps = expressions_1[1]\n",
    "print(cat_ref, cat_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=1.0, recall=1.0, fmeasure=1.0) Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "Score(precision=1.0, recall=0.3333333333333333, fmeasure=0.5) Score(precision=1.0, recall=0.3333333333333333, fmeasure=0.5)\n",
      "Score(precision=1.0, recall=0.2, fmeasure=0.33333333333333337) Score(precision=1.0, recall=0.2, fmeasure=0.33333333333333337)\n",
      "0.7074074074074074\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "for i in cat_ref:\n",
    "    score = scorer.score(i[0], cat_exps[1])\n",
    "    print(score['rouge1'], score['rougeL'])\n",
    "    test_scores.append(score['rouge1'])\n",
    "print(np.average(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "rouge1_scores = []\n",
    "rougeL_scores = []\n",
    "for start in files:\n",
    "    references = np.load(f'references_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    expressions = np.load(f'top3_exps_from_{start}_to_{start+1000}.npy', allow_pickle=True)\n",
    "    for ref, exps in zip(references, expressions):\n",
    "        item_reference = [word_tokenize(r[0].lower()) for r in ref]\n",
    "        exp_recalls_rouge_1 = []\n",
    "        exp_recalls_rouge_L = []\n",
    "        for exp in exps:\n",
    "            exp_recalls_rouge_1.append(np.average([scorer.score(target[0], exp)['rouge1'].recall for target in ref]))\n",
    "            exp_recalls_rouge_L.append(np.average([scorer.score(target[0], exp)['rougeL'].recall for target in ref]))\n",
    "        rouge1_scores.append(max(exp_recalls_rouge_1))\n",
    "        rougeL_scores.append(max(exp_recalls_rouge_L))\n",
    "print(len(rouge1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24809894991750528 0.16986040769537178\n",
      "0.1687072859439036 0.18169670240599997\n"
     ]
    }
   ],
   "source": [
    "nonzero_rouge1 = [score for score in rouge1_scores if score != 0]\n",
    "print(np.average(nonzero_rouge1), np.std(nonzero_rouge1))\n",
    "print(np.average(rouge1_scores), np.std(rouge1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2480695381527994 0.1698341010559976\n",
      "0.1686872859439036 0.18167124031241783\n"
     ]
    }
   ],
   "source": [
    "nonzero_rougeL = [score for score in rougeL_scores if score != 0]\n",
    "print(np.average(nonzero_rougeL), np.std(nonzero_rougeL))\n",
    "print(np.average(rougeL_scores), np.std(rougeL_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "references = np.load('references_from_0_to_1000.npy', allow_pickle=True)\n",
    "expressions_1 = np.load('top3_exps_from_0_to_1000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add meteor metric (include synonims)\n",
    "#### Improve matching algorithm (prioritizing matched objects with synonims)\n",
    "#### Add relations to RSA ?\n",
    "#### Generate attributes/objects/relations from scenegraph on the training dataset.\n",
    "#### Learn threshold (entropy & for relations?) from the training data\n",
    "#### Idea: treating nodes as objects & edges as relations. -> generate expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767\n",
      "[0, 2, 4, 9, 12, 14, 20, 21, 24, 27]\n",
      "1600\n",
      "[0, 9, 12, 14, 21, 24, 27, 30, 35, 36]\n",
      "1600\n",
      "[0, 9, 12, 14, 21, 24, 27, 30, 35, 36]\n"
     ]
    }
   ],
   "source": [
    "# FINDING ALL IMAGES WITH 0 SCORE EXPRESSION\n",
    "bad_bleu_score_images = []\n",
    "for i in range(len(bleu_score)):\n",
    "    if bleu_score[i] == 0:\n",
    "        bad_bleu_score_images.append(i)\n",
    "print(len(bad_bleu_score_images))\n",
    "print(bad_bleu_score_images[:10])\n",
    "\n",
    "bad_rouge1_score_images = []\n",
    "bad_rougeL_score_images = []\n",
    "for i in range(len(rouge1_scores)):\n",
    "    if rouge1_scores[i] == 0:\n",
    "        bad_rouge1_score_images.append(i)\n",
    "    if rougeL_scores[i] == 0:\n",
    "        bad_rougeL_score_images.append(i)\n",
    "print(len(bad_rouge1_score_images))\n",
    "print(bad_rouge1_score_images[:10])\n",
    "print(len(bad_rougeL_score_images))\n",
    "print(bad_rougeL_score_images[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "1600\n",
      "1767\n"
     ]
    }
   ],
   "source": [
    "rouge_bad_intersection = np.intersect1d(bad_rougeL_score_images, bad_bleu_score_images)\n",
    "print(len(rouge_bad_intersection))\n",
    "bleu_rouge_bad_intersection = np.intersect1d(bad_rougeL_score_images, bad_bleu_score_images)\n",
    "print(len(bleu_rouge_bad_intersection))\n",
    "bleu_rouge_bad_union = np.union1d(bad_rougeL_score_images, bad_bleu_score_images)\n",
    "print(len(bleu_rouge_bad_union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bad_rouge_score_reference_indexes.npy',bad_rouge1_score_images)\n",
    "np.save('bad_bleu_score_reference_indexes.npy',bad_bleu_score_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
