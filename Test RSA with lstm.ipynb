{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "data_path = config['data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('three_gram_lstm_loss_2.8469_accuracy_0.4227.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat = np.load('train_label_tokenized.npy', allow_pickle=True)\n",
    "test_dat = np.load('test_label_tokenized.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_data = np.load('combined_label_tokenized.npy', allow_pickle=True)\n",
    "combined_data = list(combined_data)\n",
    "\n",
    "vocab = set()\n",
    "for sent in combined_data:\n",
    "    for w in sent:\n",
    "        vocab.add(w.lower())\n",
    "vocab.add('')\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "train_dat = list(train_dat)\n",
    "train_data = []\n",
    "for data_point in train_dat:\n",
    "    d_point = [w.lower() for w in data_point[::-1]]\n",
    "    train_data.append(d_point)\n",
    "test_dat = list(test_dat)\n",
    "test_data = []\n",
    "for data_point in test_dat:\n",
    "    d_point = [w.lower() for w in data_point[::-1]]\n",
    "    test_data.append(d_point)\n",
    "\n",
    "processed_data = []\n",
    "for data_point in combined_data:\n",
    "    processed_data_point = [w.lower() for w in data_point[::-1]]\n",
    "    processed_data.append(processed_data_point)\n",
    "\n",
    "two_gram_train = []\n",
    "for sentence in train_data:\n",
    "    sentence.insert(0, '')\n",
    "    sentence.append('')\n",
    "    for i in range(len(sentence)-2):\n",
    "        two_gram_train.append([sentence[i:i+2], sentence[i+2]])\n",
    "two_gram_test = []\n",
    "for sentence in test_data:\n",
    "    sentence.insert(0, '')\n",
    "    sentence.append('')\n",
    "    for i in range(len(sentence)-2):\n",
    "        two_gram_test.append([sentence[i:i+2], sentence[i+2]])\n",
    "\n",
    "word_to_idx = {}\n",
    "for sentence in processed_data:\n",
    "    for word in sentence:\n",
    "        if word.lower() not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "word_to_idx[''] = len(word_to_idx)\n",
    "\n",
    "two_gram_train_inputs = []\n",
    "two_gram_train_outputs = []\n",
    "for sent, next_word in two_gram_train:\n",
    "    sentence_in = np.array([word_to_idx[w] for w in sent])\n",
    "    two_gram_train_inputs.append(sentence_in)\n",
    "    next_word_out = np.array([word_to_idx[next_word]])\n",
    "    two_gram_train_outputs.append(next_word_out)\n",
    "two_gram_train_inputs = np.array(two_gram_train_inputs)\n",
    "two_gram_train_outputs = to_categorical(two_gram_train_outputs, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_gram_train = []\n",
    "for sentence in train_data:\n",
    "    sentence.insert(0, '')\n",
    "    sentence.insert(0, '')\n",
    "    sentence.append('')\n",
    "    for i in range(len(sentence)-3):\n",
    "        three_gram_train.append([sentence[i:i+3], sentence[i+3]])\n",
    "three_gram_test = []\n",
    "for sentence in test_data:\n",
    "    sentence.insert(0, '')\n",
    "    sentence.insert(0, '')\n",
    "    sentence.append('')\n",
    "    for i in range(len(sentence)-3):\n",
    "        three_gram_test.append([sentence[i:i+3], sentence[i+3]])\n",
    "\n",
    "three_gram_train_inputs = []\n",
    "three_gram_train_outputs = []\n",
    "for sent, next_word in three_gram_train:\n",
    "    sentence_in = np.array([word_to_idx[w] for w in sent])\n",
    "    three_gram_train_inputs.append(sentence_in)\n",
    "    next_word_out = np.array([word_to_idx[next_word]])\n",
    "    three_gram_train_outputs.append(next_word_out)\n",
    "three_gram_train_inputs = np.array(three_gram_train_inputs)\n",
    "three_gram_train_outputs = to_categorical(three_gram_train_outputs, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = {}\n",
    "for word in word_to_idx:\n",
    "    idx = word_to_idx[word]\n",
    "    idx_to_word[idx] = word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 140228 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 236978 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 297067 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 226930 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 362567 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 327061 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 276977 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 218941 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 89325 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 236814 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 176466 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 367680 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 241366 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 196275 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 302921 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 16054 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 93165 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 139883 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 152657 : accuracy 0.0, top5 accuracy: 0.0\n",
      "checkpoint 17942 : accuracy 0.0, top5 accuracy: 0.0\n",
      "accuracy 0.0, top5 accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "top5 = 0\n",
    "top1 = 0\n",
    "for _ in range(1000):\n",
    "    i = np.random.randint(len(two_gram_train_inputs))\n",
    "    out = new_model.predict(two_gram_train_inputs[i])\n",
    "    next_words = out[0].argsort()[-5:][::-1]\n",
    "    label = np.where(two_gram_train_outputs[i] == 1)[0][0]\n",
    "    if label == next_words[0]:\n",
    "        top1 += 1\n",
    "    if label in next_words:\n",
    "        top5 += 1\n",
    "    if _ % 50 == 49:\n",
    "        print(f'checkpoint {i} : accuracy {top1/_}, top5 accuracy: {top5/_}')\n",
    "print(f'accuracy {top1/1000}, top5 accuracy: {top5/1000}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "# print(three_gram_train_inputs[34780])\n",
    "# print(three_gram_train_inputs[34780].shape)\n",
    "# print(type(three_gram_train_inputs[34780]))\n",
    "test = np.array([[9904, 9904,   44]])\n",
    "print(test.shape)\n",
    "out = new_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9905)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9904  True 9904\n",
      "8 left True 8\n",
      "25 right True 25\n",
      "23 little True 23\n",
      "11 the True 11\n",
      "3.6278555e-10\n"
     ]
    }
   ],
   "source": [
    "next_words = out[0].argsort()[-5:][::-1]\n",
    "for i in next_words:\n",
    "    word = idx_to_word[str(i)]\n",
    "    print(i,word, word in vocab, word_to_idx[word])\n",
    "print(out[0][3100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  46,   29, 1655])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_gram_train_inputs[34780]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "bottom\n"
     ]
    }
   ],
   "source": [
    "index = np.where(two_gram_train_outputs[15604] == 1)[0][0]\n",
    "print(index)\n",
    "print(idx_to_word[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_idx_vice_versa.json', 'w') as fp:\n",
    "    saved_token_to_id_and_reverse = {\n",
    "        'word_to_idx': word_to_idx,\n",
    "        'idx_to_word': idx_to_word\n",
    "    }\n",
    "    json.dump(saved_token_to_id_and_reverse, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('word_to_idx_vice_versa.json')\n",
    "tokenizer = json.load(f)\n",
    "word_to_idx = tokenizer['word_to_idx']\n",
    "idx_to_word = tokenizer['idx_to_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx['zebra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CENTER SCATEBOARDER', 'person in middle', 'the person in the middle']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from helper import *\n",
    "import argparse\n",
    "from rsa import RSA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "data_path = config['data_path']\n",
    "\n",
    "\n",
    "file_id = 21540#3278#182\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_path,f'refCOCO/train/attr_tables/attr_{file_id}.tsv'), encoding='utf-8',sep='\\t')\n",
    "\n",
    "with open(os.path.join(data_path,f'refCOCO/train/labels/lab_{file_id}.json')) as json_file:\n",
    "    label = json.load(json_file)\n",
    "refs = [[r] for r in label['ref_sents']]\n",
    "img_id = df['image_id'][0]\n",
    "filename = os.path.join(data_path, f'refCOCO/train/imgs_by_id/{img_id}.jpg')\n",
    "image = plt.imread(filename)\n",
    "# get relations generated from graph faster-RCNN\n",
    "rel_load = np.load(f'./train_relation_extraction.npy', allow_pickle=True)\n",
    "generated_relations = rel_load[file_id]\n",
    "\n",
    "# add lstm model to rsa.\n",
    "lstm = tf.keras.models.load_model('three_gram_lstm_loss_2.8469_accuracy_0.4227.h5')\n",
    "\n",
    "box_data = df[['box_alias', 'x1','y1','w','h']]\n",
    "fig,ax = plt.subplots(1)\n",
    "img = image\n",
    "\n",
    "# ax.imshow(img)\n",
    "rng = [i for i in range(len(box_data))]\n",
    "for i in [4]:#rng[:]:\n",
    "    name, x,y,w,h = list(box_data.iloc[i,:])\n",
    "    ax = draw_box_obj(name,x,y,w,h,img,ax)\n",
    "\n",
    "print(label['ref_sents'])\n",
    "bbox = label['bbox'][0]\n",
    "sentence = label['ref_sents'][0]\n",
    "fig,ax_true_label = plt.subplots(1)\n",
    "ax_true_label.imshow(img)\n",
    "draw_box_obj(sentence,bbox[0],bbox[1],bbox[2],bbox[3],img,ax_true_label)\n",
    "\n",
    "rsa_agent = RSA(df, generated_relations=generated_relations, \\\n",
    "                model=lstm, word_to_idx=word_to_idx, idx_to_word=idx_to_word)\n",
    "\n",
    "# output = rsa_agent.full_speaker('woman-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/hle/git/rsa_refer/rsa.py:137: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return result/np.sum(result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27857507409142507 girl\n",
      "0.06629854490214701 the first from right\n"
     ]
    }
   ],
   "source": [
    "output = rsa_agent.full_speaker('woman-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['girl', 'the first from right']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(10, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method numpy in module tensorflow.python.framework.ops:\n",
      "\n",
      "numpy() method of tensorflow.python.framework.ops.EagerTensor instance\n",
      "    Copy of the contents of this Tensor into a NumPy array or scalar.\n",
      "    \n",
      "    Unlike NumPy arrays, Tensors are immutable, so this method has to copy\n",
      "    the contents to ensure safety. Use `memoryview` to get a readonly\n",
      "    view of the contents without doing a copy:\n",
      "    \n",
      "    >>> t = tf.constant([42])\n",
      "    >>> np.array(memoryview(t))\n",
      "    array([42], dtype=int32)\n",
      "    \n",
      "    Note that `memoryview` is only zero-copy for Tensors on CPU. If a Tensor\n",
      "    is on GPU, it will have to be transferred to CPU first in order for\n",
      "    `memoryview` to work.\n",
      "    \n",
      "    Returns:\n",
      "      A NumPy array of the same shape and dtype or a NumPy scalar, if this\n",
      "      Tensor has rank 0.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If the dtype of this Tensor does not have a compatible\n",
      "        NumPy dtype.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(x.numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
